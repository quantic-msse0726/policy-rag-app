# AI Tooling

This project was built with AI-assisted development tools, with all outputs reviewed and adjusted manually.

## Tools Used

- Codex/GPT-based coding assistant for implementation support, debugging, and iterative refactoring.
- LLM assistance for drafting synthetic policy corpus content and test question ideas.

## How AI Was Used

- Scaffolded initial FastAPI app structure and RAG pipeline modules.
- Iterated on ingestion logic for mixed file formats (Markdown, TXT, HTML, PDF).
- Improved retrieval/prompting flow and citation formatting in responses.
- Helped write smoke tests and CI workflow checks.
- Assisted with evaluation script design for groundedness, citation accuracy, and latency metrics.

## What Worked Well

- Rapid generation of boilerplate and repetitive code sections.
- Faster iteration on prompt and retrieval heuristics.
- Quick drafting of documentation and experiment notes.

## What Did Not Work Well

- Some generated code required corrections for edge cases and robustness.
- Prompt/text output formatting needed manual cleanup for consistency.
- Evaluation logic still required human validation to ensure metric definitions matched project goals.

## Validation and Oversight

All AI-generated or AI-assisted code and text was reviewed, edited, and tested by the project author before inclusion.
